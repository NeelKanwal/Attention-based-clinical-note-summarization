{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled16.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "410361e295d6499999aff59869011a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9de531c1edb34977ad83c5b502d52806",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb1b29b4645947f39d313d4557e8fabf",
              "IPY_MODEL_c1fae5d5721e473b974cd18a9a2b4122"
            ]
          }
        },
        "9de531c1edb34977ad83c5b502d52806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb1b29b4645947f39d313d4557e8fabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fa58a57488b84c9a80be167af32b5449",
            "_dom_classes": [],
            "description": "Epoch",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1e60787b5264734ad0d5cac8c6fd755"
          }
        },
        "c1fae5d5721e473b974cd18a9a2b4122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67a3bdd92d9c42e0846773b2948785e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 3/3 [2:08:40&lt;00:00, 2573.39s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6158c38de7e842af923a9a2fb1fed4ef"
          }
        },
        "fa58a57488b84c9a80be167af32b5449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1e60787b5264734ad0d5cac8c6fd755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67a3bdd92d9c42e0846773b2948785e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6158c38de7e842af923a9a2fb1fed4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a2a924a8a824476acb441237daa7726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03bdcf59cd17430c9509206a5dbc4c9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7113feaa04964f948e74b8d8c40d5042",
              "IPY_MODEL_c5689b190acf420996e44c3e322e2cea"
            ]
          }
        },
        "03bdcf59cd17430c9509206a5dbc4c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7113feaa04964f948e74b8d8c40d5042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b15f4579cc7649f3bcc401b6f988c723",
            "_dom_classes": [],
            "description": "Current iteration",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 5966,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5966,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5a5ed2f0e1042068ee58a5170b9dcc9"
          }
        },
        "c5689b190acf420996e44c3e322e2cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6e99847e64e9477588cdae97fc21fc42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 5966/5966 [42:50&lt;00:00,  2.65it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_546b4dd9820048d29f49433f05947ca4"
          }
        },
        "b15f4579cc7649f3bcc401b6f988c723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5a5ed2f0e1042068ee58a5170b9dcc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e99847e64e9477588cdae97fc21fc42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "546b4dd9820048d29f49433f05947ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f757b99f8144b6bab5f75b05b9c0149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_337e55b4a6494a539f3eed42dca5c7ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd653f36dadb415daae519ecea38f189",
              "IPY_MODEL_1ad48dc425d348cd8a7a6d2e51c9be64"
            ]
          }
        },
        "337e55b4a6494a539f3eed42dca5c7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd653f36dadb415daae519ecea38f189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_21feefd16b3748e0860560e99d736ed9",
            "_dom_classes": [],
            "description": "Current iteration",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 5966,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5966,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bae41276934b415a8deb3abd13e3ddbf"
          }
        },
        "1ad48dc425d348cd8a7a6d2e51c9be64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47808f5ae25a4c0f9fc65ca95c50aaa0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 5966/5966 [42:52&lt;00:00,  2.66it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e8324c9c9f2408ab60d08d56eaf3c3b"
          }
        },
        "21feefd16b3748e0860560e99d736ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bae41276934b415a8deb3abd13e3ddbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47808f5ae25a4c0f9fc65ca95c50aaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e8324c9c9f2408ab60d08d56eaf3c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd17b551b7914ab68e7f7d3cb365b191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f0d6057ffa68415a94caf2d142a4edce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ccfc9c89d4c6463a98f49aed5b4a7445",
              "IPY_MODEL_069afd626d3c4bb99e5c5e9bf8a4a261"
            ]
          }
        },
        "f0d6057ffa68415a94caf2d142a4edce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ccfc9c89d4c6463a98f49aed5b4a7445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4963602735854e8383dacd1bc4e3db55",
            "_dom_classes": [],
            "description": "Current iteration",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 5966,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5966,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9433a5a8cbc4f5c93e52c6277dfa85d"
          }
        },
        "069afd626d3c4bb99e5c5e9bf8a4a261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fce5cf1b410e43fba1bcf85ebf06e84d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 5966/5966 [42:52&lt;00:00,  2.65it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2c6ac9f02f74b5482f4ef25f71f51c4"
          }
        },
        "4963602735854e8383dacd1bc4e3db55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9433a5a8cbc4f5c93e52c6277dfa85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fce5cf1b410e43fba1bcf85ebf06e84d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2c6ac9f02f74b5482f4ef25f71f51c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeelKanwal/BERTOLOGY-Based-Extractive-Summarization-for-Clinical-Notes/blob/master/FineTunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6DbcOsQXvW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# just run this once to upgrade the RAM for Colab\n",
        "a = []\n",
        "while(1):\n",
        "    a.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULjgZvt4wfTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f648e614-fe81-4391-db2f-ca450f80e1ae"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex.git && cd apex && python setup.py install --cuda_ext --cpp_ext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 6777 (delta 17), reused 8 (delta 5), pack-reused 6745\u001b[K\n",
            "Receiving objects: 100% (6777/6777), 13.76 MiB | 7.03 MiB/s, done.\n",
            "Resolving deltas: 100% (4523/4523), done.\n",
            "torch.__version__  =  1.5.0+cu101\n",
            "setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "  warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "Compiling cuda extensions with\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "from /usr/local/cuda/bin\n",
            "\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating apex.egg-info\n",
            "writing apex.egg-info/PKG-INFO\n",
            "writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "writing top-level names to apex.egg-info/top_level.txt\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:304: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/apex\n",
            "copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "running build_ext\n",
            "building 'apex_C' extension\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/csrc\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[Kcsrc/flatten_unflatten.cpp:2:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:\u001b[m\u001b[K In member function â€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& torch::utils::TensorGroup::type()\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     return tensors[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/flatten_unflatten.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'amp_C' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'syncbn' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'fused_layer_norm_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:129:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(input);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:149:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(input);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:150:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(gamma);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:151:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(beta);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kat::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:193:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(dout);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:194:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(mean);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:195:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(invvar);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:196:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(input);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:218:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(dout);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:219:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(mean);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:220:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(invvar);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:221:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(input);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:222:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(gamma);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:42:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            " #define CHECK_CUDA(x) TORCH_CHECK(x.type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K.is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY\u001b[m\u001b[Kâ€™\n",
            " #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(\u001b[01;36m\u001b[Kexpr\u001b[m\u001b[K), 0))\n",
            "                                                                 \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[Kâ€™\n",
            "   if (\u001b[01;36m\u001b[KC10_UNLIKELY_OR_CONST\u001b[m\u001b[K(!(cond))) {                     \\\n",
            "       \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[Kâ€™\n",
            " #define TORCH_CHECK(cond, ...) \u001b[01;36m\u001b[KTORCH_CHECK_WITH\u001b[m\u001b[K(Error, cond, __VA_ARGS__)\n",
            "                                \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:117:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KTORCH_CHECK\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_CUDA(x) \u001b[01;36m\u001b[KTORCH_CHECK\u001b[m\u001b[K(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:119:24:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_CUDA\u001b[m\u001b[Kâ€™\n",
            " #define CHECK_INPUT(x) \u001b[01;36m\u001b[KCHECK_CUDA\u001b[m\u001b[K(x); CHECK_CONTIGUOUS(x)\n",
            "                        \u001b[01;36m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:223:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KCHECK_INPUT\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KC\u001b[m\u001b[KHECK_INPUT(beta);\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/layer_norm_cuda.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "building 'mlp_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:56:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "                   \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:64:77:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                             \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:65:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   auto reserved_space = at::empty({reserved_size}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:65:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[Kreserved_size\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "   auto reserved_space = at::empty({reserved_size}, inputs[0].type()\u001b[01;35m\u001b[K)\u001b[m\u001b[K;\n",
            "                                                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:65:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[Kreserved_size\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"mlp_forward\", [&] {\n",
            "                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                             \\\n",
            "                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:70:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < n\u001b[m\u001b[Kum_layers; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:76:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     auto \u001b[01;35m\u001b[Kr\u001b[m\u001b[Kesult = mlp_fp<scalar_t>(\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:70:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < n\u001b[m\u001b[Kum_layers; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:76:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     auto \u001b[01;35m\u001b[Kr\u001b[m\u001b[Kesult = mlp_fp<scalar_t>(\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:70:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < n\u001b[m\u001b[Kum_layers; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:76:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     auto \u001b[01;35m\u001b[Kr\u001b[m\u001b[Kesult = mlp_fp<scalar_t>(\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In function â€˜\u001b[01m\u001b[Kstd::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)\u001b[m\u001b[Kâ€™:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:113:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (int i = 0; \u001b[01;35m\u001b[Ki < num_layers\u001b[m\u001b[K; i++) {\n",
            "                   \u001b[01;35m\u001b[K~~^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:119:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "   for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "                   \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:120:67:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K));  // clone for testing now\n",
            "                                                                   \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K, \"mlp_backward\", [&] {\n",
            "                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "     const auto& the_type = \u001b[01;36m\u001b[KTYPE\u001b[m\u001b[K;                                             \\\n",
            "                            \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[Kâ€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     at::ScalarType _st = ::detail::scalar_type(the_type\u001b[01;35m\u001b[K)\u001b[m\u001b[K;                    \\\n",
            "                                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            " inline at::ScalarType \u001b[01;36m\u001b[Kscalar_type\u001b[m\u001b[K(const at::DeprecatedTypeProperties &t) {\n",
            "                       \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:125:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < n\u001b[m\u001b[Kum_layers; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:129:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:80:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({\u001b[01;35m\u001b[Kwork_size / sizeof(\u001b[m\u001b[Kscalar_t)}, inputs[0].type());\n",
            "                                  \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({\u001b[01;35m\u001b[Kwork_size / sizeof(\u001b[m\u001b[Kscalar_t)}, inputs[0].type());\n",
            "                                  \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:139:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     auto \u001b[01;35m\u001b[Kr\u001b[m\u001b[Kesult = mlp_bp<scalar_t>(\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:125:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < n\u001b[m\u001b[Kum_layers; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:129:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:80:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({\u001b[01;35m\u001b[Kwork_size / sizeof(\u001b[m\u001b[Kscalar_t)}, inputs[0].type());\n",
            "                                  \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({\u001b[01;35m\u001b[Kwork_size / sizeof(\u001b[m\u001b[Kscalar_t)}, inputs[0].type());\n",
            "                                  \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:139:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     auto \u001b[01;35m\u001b[Kr\u001b[m\u001b[Kesult = mlp_bp<scalar_t>(\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:125:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < n\u001b[m\u001b[Kum_layers; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:129:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     for (int i = 0; \u001b[01;35m\u001b[Ki < inputs.size()\u001b[m\u001b[K; i++) {\n",
            "                     \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:80:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kâ€˜\u001b[01m\u001b[Kat::DeprecatedTypeProperties& at::Tensor::type() const\u001b[m\u001b[Kâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [\u001b[01;35m\u001b[K-Wdeprecated-declarations\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\u001b[01;35m\u001b[K)\u001b[m\u001b[K);\n",
            "                                                                                \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   DeprecatedTypeProperties & \u001b[01;36m\u001b[Ktype\u001b[m\u001b[K() const {\n",
            "                              \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({\u001b[01;35m\u001b[Kwork_size / sizeof(\u001b[m\u001b[Kscalar_t)}, inputs[0].type());\n",
            "                                  \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of â€˜\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[Kâ€™ from â€˜\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[Kâ€™ to â€˜\u001b[01m\u001b[Klong int\u001b[m\u001b[Kâ€™ inside { } [\u001b[01;35m\u001b[K-Wnarrowing\u001b[m\u001b[K]\n",
            "     auto work_space = at::empty({\u001b[01;35m\u001b[Kwork_size / sizeof(\u001b[m\u001b[Kscalar_t)}, inputs[0].type());\n",
            "                                  \u001b[01;35m\u001b[K~~~~~~~~~~^~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:139:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable â€˜\u001b[01m\u001b[Kresult\u001b[m\u001b[Kâ€™ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n",
            "     auto \u001b[01;35m\u001b[Kr\u001b[m\u001b[Kesult = mlp_bp<scalar_t>(\n",
            "          \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro â€˜\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[Kâ€™\n",
            "     return \u001b[01;36m\u001b[K__VA_ARGS__\u001b[m\u001b[K();                          \\\n",
            "            \u001b[01;36m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro â€˜\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[Kâ€™\n",
            "   \u001b[01;36m\u001b[KA\u001b[m\u001b[KT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "   \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++14\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n",
            "\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
            "creating build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
            "creating build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
            "creating build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
            "creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
            "copying build/lib.linux-x86_64-3.6/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n",
            "creating build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof\n",
            "creating build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
            "creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
            "creating build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
            "creating build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
            "creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
            "copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "creating stub loader for apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "creating stub loader for amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "creating stub loader for syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "creating stub loader for fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating stub loader for mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/apex_C.py to apex_C.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/amp_C.py to amp_C.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/syncbn.py to syncbn.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/fused_layer_norm_cuda.py to fused_layer_norm_cuda.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mlp_cuda.py to mlp_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.amp_C.cpython-36: module references __file__\n",
            "__pycache__.apex_C.cpython-36: module references __file__\n",
            "__pycache__.fused_layer_norm_cuda.cpython-36: module references __file__\n",
            "__pycache__.mlp_cuda.cpython-36: module references __file__\n",
            "__pycache__.syncbn.cpython-36: module references __file__\n",
            "apex.pyprof.nvtx.__pycache__.nvmarker.cpython-36: module references __file__\n",
            "apex.pyprof.nvtx.__pycache__.nvmarker.cpython-36: module references __path__\n",
            "creating dist\n",
            "creating 'dist/apex-0.1-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing apex-0.1-py3.6-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg\n",
            "Extracting apex-0.1-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "apex 0.1 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for apex==0.1\n",
            "Finished processing dependencies for apex==0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MprsW97LYZqm",
        "colab_type": "code",
        "outputId": "c8d11070-c676-4b5d-9269-db5755629aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardx\n",
        "!pip install seqeval\n",
        "!pip install simpletransformers\n",
        "!pip install jsonlines"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.18.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx) (46.3.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.4)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.6/dist-packages (0.28.5)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.0.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.4)\n",
            "Requirement already satisfied: transformers>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.9.1)\n",
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.4.5.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->simpletransformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->simpletransformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->simpletransformers) (0.1.90)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.1->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.3.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihGsFJ3-G4Bo",
        "colab_type": "code",
        "outputId": "fbf73491-ad90-4717-bb0b-2473d8ce1e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!pip install apex"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: apex in /usr/local/lib/python3.6/dist-packages (0.9.10.dev0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from apex) (2.23.0)\n",
            "Requirement already satisfied: zope.sqlalchemy in /usr/local/lib/python3.6/dist-packages (from apex) (1.3)\n",
            "Requirement already satisfied: velruse>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from apex) (1.1.1)\n",
            "Requirement already satisfied: pyramid-mailer in /usr/local/lib/python3.6/dist-packages (from apex) (0.15.1)\n",
            "Requirement already satisfied: wtforms in /usr/local/lib/python3.6/dist-packages (from apex) (2.3.1)\n",
            "Requirement already satisfied: wtforms-recaptcha in /usr/local/lib/python3.6/dist-packages (from apex) (0.3.2)\n",
            "Requirement already satisfied: cryptacular in /usr/local/lib/python3.6/dist-packages (from apex) (1.5.5)\n",
            "Requirement already satisfied: pyramid>1.1.2 in /usr/local/lib/python3.6/dist-packages (from apex) (1.10.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->apex) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->apex) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->apex) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->apex) (2020.4.5.1)\n",
            "Requirement already satisfied: SQLAlchemy>=0.7 in /usr/local/lib/python3.6/dist-packages (from zope.sqlalchemy->apex) (1.3.16)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.sqlalchemy->apex) (46.3.0)\n",
            "Requirement already satisfied: zope.interface>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from zope.sqlalchemy->apex) (5.1.0)\n",
            "Requirement already satisfied: transaction>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from zope.sqlalchemy->apex) (3.0.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from velruse>=1.0.3->apex) (1.3.0)\n",
            "Requirement already satisfied: anykeystore in /usr/local/lib/python3.6/dist-packages (from velruse>=1.0.3->apex) (0.2)\n",
            "Requirement already satisfied: python3-openid in /usr/local/lib/python3.6/dist-packages (from velruse>=1.0.3->apex) (3.1.0)\n",
            "Requirement already satisfied: repoze.sendmail>=4.1 in /usr/local/lib/python3.6/dist-packages (from pyramid-mailer->apex) (4.4.1)\n",
            "Requirement already satisfied: MarkupSafe in /usr/local/lib/python3.6/dist-packages (from wtforms->apex) (1.1.1)\n",
            "Requirement already satisfied: pbkdf2 in /usr/local/lib/python3.6/dist-packages (from cryptacular->apex) (1.3)\n",
            "Requirement already satisfied: hupper>=1.5 in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (1.10.2)\n",
            "Requirement already satisfied: webob>=1.8.3 in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (1.8.6)\n",
            "Requirement already satisfied: zope.deprecation>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (4.4.0)\n",
            "Requirement already satisfied: plaster in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (1.0)\n",
            "Requirement already satisfied: translationstring>=0.4 in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (1.3)\n",
            "Requirement already satisfied: venusian>=1.0 in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (3.0.0)\n",
            "Requirement already satisfied: plaster-pastedeploy in /usr/local/lib/python3.6/dist-packages (from pyramid>1.1.2->apex) (0.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from python3-openid->velruse>=1.0.3->apex) (0.6.0)\n",
            "Requirement already satisfied: PasteDeploy>=2.0 in /usr/local/lib/python3.6/dist-packages (from plaster-pastedeploy->pyramid>1.1.2->apex) (2.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egRkbhKPqWiv",
        "colab_type": "code",
        "outputId": "1432fc44-ada9-4ff9-9016-9df3a03c3742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOv_a2bmHAF0",
        "colab_type": "code",
        "outputId": "c27c29c7-8cbc-4723-b020-294e836343ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "device = torch.cuda.current_device()\n",
        "device_name = torch.cuda.get_device_name(device)\n",
        "cuda_flag = torch.cuda.is_available()\n",
        "\n",
        "print('current device='+device_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current device=Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh0PIXt7IswU",
        "colab_type": "code",
        "outputId": "9cbd0eea-73d1-4a5f-c17d-a26b04d57728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhLq-S3qYb5m",
        "colab_type": "code",
        "outputId": "1003eff9-d194-497a-ddda-1b99dad0f09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/clinical_notes_ICD9\")\n",
        "%ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mapex\u001b[0m/                                  \u001b[01;34mmimicdata\u001b[0m/\n",
            " \u001b[01;34mcache_dir\u001b[0m/                             models.py\n",
            " clinicalNLP.yml                        \u001b[01;34mnotebooks\u001b[0m/\n",
            " constants.py                           \u001b[01;34moutputs\u001b[0m/\n",
            " converge.py                            persistence.py\n",
            " \u001b[01;34mdataproc\u001b[0m/                              \u001b[01;34mpredictions\u001b[0m/\n",
            " dataproc_mimic_III.ipynb               preprocess.py\n",
            " datasets.py                            \u001b[01;34m__pycache__\u001b[0m/\n",
            " evaluation.py                          README.md\n",
            " get_metrics_for_saved_predictions.py   \u001b[01;34mruns\u001b[0m/\n",
            " ICD9_descriptions                      \u001b[01;34msaved_models\u001b[0m/\n",
            " \u001b[01;34minput_files\u001b[0m/                           training_2.py\n",
            " \u001b[01;34mlearn\u001b[0m/                                'training command.txt'\n",
            " LICENSE                                training.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD3gzWQ7bDgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import datasets\n",
        "from datasets import MimicDataset\n",
        "from datasets import collate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWO3UU-hYmHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "FILES =\"/content/drive/My Drive/clinical_notes_ICD9/input_files/\"\n",
        "\n",
        "args = argparse.Namespace()\n",
        "\n",
        "args.data_path = FILES + 'notes_labeled_train.ndjson'\n",
        "args.Y=\"full\"\n",
        "args.vocab = FILES + 'vocab.csv'\n",
        "args.data_dir = '/content/drive/My Drive/data'\n",
        "args.embed_desc = True\n",
        "args.exclude_non_billable = False\n",
        "args.include_invalid = False\n",
        "args.max_len=3000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_5XEMLXYqe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dicts = datasets.load_lookups(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I-qrCISe3D-",
        "colab_type": "code",
        "outputId": "827605e9-28f3-48ac-9e1f-711d612a240f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "num_labels_fine = len(dicts['ind2c'])\n",
        "num_labels_coarse = len(dicts['ind2c_coarse'])\n",
        "print(num_labels_fine)\n",
        "print(num_labels_coarse)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8924\n",
            "1167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x832H8nPfTA0",
        "colab_type": "code",
        "outputId": "d59b22df-6a88-4a36-d8af-8a135084fd30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset_train = MimicDataset(args.data_path, dicts, num_labels_fine, num_labels_coarse, args.max_len) # data_id,target_fine,target_coarse,hadm_id,doc\n",
        "dataset_dev = MimicDataset(args.data_path.replace('train', 'dev'), dicts, num_labels_fine, num_labels_coarse, args.max_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data from /content/drive/My Drive/clinical_notes_ICD9/input_files/notes_labeled_train.ndjson\n",
            "loading data from /content/drive/My Drive/clinical_notes_ICD9/input_files/notes_labeled_dev.ndjson\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccLonaO0xq-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "train=[data for data in dataset_train]\n",
        "train=[[\" \".join(data[4]),data[2].cpu().detach().numpy()] for data in train]\n",
        "test=[data for data in dataset_dev]\n",
        "test=[[\" \".join(data[4]),data[2].cpu().detach().numpy()] for data in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gelGK0aZhgR9",
        "colab_type": "code",
        "outputId": "da726191-e1fa-45f4-ef73-67f0b3b81529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_test=pd.DataFrame(data=test,columns=[\"text\",\"label\"])\n",
        "df_test[\"label\"] = df_test[\"label\"].apply(lambda val: val.astype(int))\n",
        "df_test.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admission date december discharge date decembe...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>admission date may discharge date may date of ...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>admission date july discharge date july date o...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>admission date april discharge date may date o...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>admission date june discharge date june date o...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              label\n",
              "0  admission date december discharge date decembe...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  admission date may discharge date may date of ...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2  admission date july discharge date july date o...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3  admission date april discharge date may date o...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4  admission date june discharge date june date o...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzsxS3lq7K61",
        "colab_type": "code",
        "outputId": "b8d111d9-efb5-4d6b-e4cb-f10dd5d60c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train=pd.DataFrame(data=train,columns=[\"text\",\"label\"])\n",
        "df_train[\"label\"] = df_train[\"label\"].apply(lambda val: val.astype(int))\n",
        "df_train.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admission date october discharge date october ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>admission date march discharge date march date...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>admission date may discharge date june date of...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>admission date november discharge date novembe...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>admission date june discharge date july date o...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                              label\n",
              "0  admission date october discharge date october ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  admission date march discharge date march date...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2  admission date may discharge date june date of...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3  admission date november discharge date novembe...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4  admission date june discharge date july date o...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pqykla1Q5HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "  \"output_dir\": \"/content/outputs/\",\n",
        "  \"cache_dir\": \"/content/cache/\",\n",
        "\n",
        "  \"fp16\": False,\n",
        "  \"fp16_opt_level\": \"O1\",\n",
        "  \"max_seq_length\": 2048,\n",
        "  \"train_batch_size\": 8,\n",
        "  \"eval_batch_size\": 8,\n",
        "  \"gradient_accumulation_steps\": 1,\n",
        "  \"num_train_epochs\": 3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"learning_rate\": 3e-5,\n",
        "  \"adam_epsilon\": 1e-8,\n",
        "  \"warmup_ratio\": 0.06,\n",
        "  \"warmup_steps\": 0,\n",
        "  \"max_grad_norm\": 1.0,\n",
        "  \"do_lower_case\": False,\n",
        "\n",
        "  \"logging_steps\": 50,\n",
        "  \"evaluate_during_training\": False,\n",
        "  \"evaluate_during_training_steps\": 2000,\n",
        "  \"evaluate_during_training_verbose\": False,\n",
        "  \"use_cached_eval_features\": True,\n",
        "  \"save_eval_checkpoints\": True,\n",
        "  \"save_steps\": 2000,\n",
        "  \"no_cache\": False,\n",
        "  \"save_model_every_epoch\": True,\n",
        "  \"tensorboard_dir\": None,\n",
        "\n",
        "  \"overwrite_output_dir\": False,\n",
        "  \"reprocess_input_data\": False,\n",
        "\n",
        "  \"process_count\": 1,\n",
        "  \"n_gpu\": 1,\n",
        "  \"silent\": False,\n",
        "  \"use_multiprocessing\": True,\n",
        "\n",
        "  \"wandb_project\": None,\n",
        "  \"wandb_kwargs\": {},\n",
        "\n",
        "  \"use_early_stopping\": True,\n",
        "  \"early_stopping_patience\": 3,\n",
        "  \"early_stopping_delta\": 0,\n",
        "\n",
        "  \"manual_seed\": None,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hCpJqyksckH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pypi.org/project/simpletransformers/#classificationmodel\n",
        "from simpletransformers.classification import MultiLabelClassificationModel\n",
        "model = MultiLabelClassificationModel('bert', 'bert-base-uncased', num_labels=num_labels_coarse,use_cuda=cuda_flag ,args=args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmhr74JJbFdX",
        "colab_type": "code",
        "outputId": "25445e67-ab62-499f-d334-23f8afde7d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315,
          "referenced_widgets": [
            "410361e295d6499999aff59869011a91",
            "9de531c1edb34977ad83c5b502d52806",
            "cb1b29b4645947f39d313d4557e8fabf",
            "c1fae5d5721e473b974cd18a9a2b4122",
            "fa58a57488b84c9a80be167af32b5449",
            "a1e60787b5264734ad0d5cac8c6fd755",
            "67a3bdd92d9c42e0846773b2948785e8",
            "6158c38de7e842af923a9a2fb1fed4ef",
            "2a2a924a8a824476acb441237daa7726",
            "03bdcf59cd17430c9509206a5dbc4c9f",
            "7113feaa04964f948e74b8d8c40d5042",
            "c5689b190acf420996e44c3e322e2cea",
            "b15f4579cc7649f3bcc401b6f988c723",
            "a5a5ed2f0e1042068ee58a5170b9dcc9",
            "6e99847e64e9477588cdae97fc21fc42",
            "546b4dd9820048d29f49433f05947ca4",
            "8f757b99f8144b6bab5f75b05b9c0149",
            "337e55b4a6494a539f3eed42dca5c7ce",
            "dd653f36dadb415daae519ecea38f189",
            "1ad48dc425d348cd8a7a6d2e51c9be64",
            "21feefd16b3748e0860560e99d736ed9",
            "bae41276934b415a8deb3abd13e3ddbf",
            "47808f5ae25a4c0f9fc65ca95c50aaa0",
            "3e8324c9c9f2408ab60d08d56eaf3c3b",
            "dd17b551b7914ab68e7f7d3cb365b191",
            "f0d6057ffa68415a94caf2d142a4edce",
            "ccfc9c89d4c6463a98f49aed5b4a7445",
            "069afd626d3c4bb99e5c5e9bf8a4a261",
            "4963602735854e8383dacd1bc4e3db55",
            "d9433a5a8cbc4f5c93e52c6277dfa85d",
            "fce5cf1b410e43fba1bcf85ebf06e84d",
            "f2c6ac9f02f74b5482f4ef25f71f51c4"
          ]
        }
      },
      "source": [
        "# Train the model\n",
        "from datetime import datetime\n",
        "start=datetime.now()\n",
        "model.train_model(df_train,show_running_loss=True)\n",
        "stop = datetime.now()\n",
        "print(\"Time Elapsed: \",stop - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/simpletransformers/classification/classification_model.py:256: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
            "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Features loaded from cache at cache_dir/cached_train_bert_128_1167_47723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "410361e295d6499999aff59869011a91",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initialâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a2a924a8a824476acb441237daa7726",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Current iteration', max=5966, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.695532"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.044585"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f757b99f8144b6bab5f75b05b9c0149",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Current iteration', max=5966, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.029490"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd17b551b7914ab68e7f7d3cb365b191",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Current iteration', max=5966, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Running loss: 0.020265\n",
            "Training of bert model complete. Saved to outputs/.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9vARxe9DqjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model\n",
        "# The default evaluation metric used is Label Ranking Average Precision (LRAP) Score.\n",
        "import sklearn\n",
        "result, model_outputs, predictions = model.eval_model(df_test,verbose=True,acc=sklearn.metrics.accuracy_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCQD_LiAodlL",
        "colab_type": "code",
        "outputId": "4add969d-9560-4f31-d8ee-62c7ba1f5de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# predictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\n",
        "len(model_outputs[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    }
  ]
}